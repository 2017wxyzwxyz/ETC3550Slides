---
title: "ETC3550 Applied&nbsp;forecasting&nbsp;for business&nbsp;and&nbsp;economics"
author: "Ch3. The forecasters' toolbox"
date: "OTexts.org/fpp3/"
toc: true
colortheme: monashwhite
output:
  binb::monash:
    fig_width: 7
    fig_height: 3.5
    includes:
      in_header: header.tex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache=TRUE)

library(tidyverse)
library(fable)
library(tsibble)
library(feasts)
library(lubridate)
library(tsibbledata)
source("nicefigs.R")

options(width=45)
```

<!-- TODO: Restructure -->
<!-- 3a: toolbox -->
<!-- 1. Tidy forecasting workflow (with progress update on stage in workflow) -->
<!-- 2. Simple forecasting methods (using model() and forecast()) -->
<!-- 3. The model definition: Transformations, adjustments and model specials -->
<!-- 4. Distributional forecasts and the prediction intervals -->

# A tidy forecasting workflow

## A tidy forecasting workflow

The process of producing forecasts can be split up into a few fundamental steps.

1. Data preparation
2. Visualise
3. Specify a model
4. Estimating the model
5. Evaluate model
6. Producing forecasts

## A tidy forecasting workflow

# Some simple forecasting methods

## Some simple forecasting methods

```{r ausbeer, fig.height=4.6, echo=FALSE}
new_production <- aus_production %>%
  filter(year(Quarter) >= 1992)
new_production %>% autoplot(Beer) +
  xlab("Year") + ylab("megalitres") +
    ggtitle("Australian quarterly beer production")
```

\begin{textblock}{7}(0.2,8.7)
\begin{alertblock}{}
\small{How would you forecast these time series?}
\end{alertblock}
\end{textblock}

## Some simple forecasting methods

```{r pigs, fig.height=4.6, echo=FALSE}
aus_livestock %>% filter(between(year(Month), 1990, 1995),
  Animal == "Pigs", State == "Victoria") %>% 
  autoplot(Count) +
  xlab("Year") + ylab("thousands") +
  ggtitle("Number of pigs slaughtered in Victoria, 1990-1995")
```

\begin{textblock}{7}(0.2,8.7)
\begin{alertblock}{}
\small{How would you forecast these time series?}
\end{alertblock}
\end{textblock}

## Some simple forecasting methods

```{r dj, fig.height=4.6, echo=FALSE}
gafa_stock %>% 
  filter(Symbol == "FB", Date >= ymd("2018-01-01")) %>% 
  autoplot(Close) +
  labs(title = "Facebook closing stock price in 2018",
       x = NULL, y = "Closing price ($USD)")
```

\begin{textblock}{7}(0.2,8.7)
\begin{alertblock}{}
\small{How would you forecast these time series?}
\end{alertblock}
\end{textblock}

## Some simple forecasting methods

\fontsize{13}{14}\sf

### Average method

  * Forecast of all future values is equal to mean of historical data $\{y_1,\dots,y_T\}$.
  * Forecasts: $\hat{y}_{T+h|T} = \bar{y} = (y_1+\dots+y_T)/T$

\pause

### Naïve method

  * Forecasts equal to last observed value.
  * Forecasts: $\hat{y}_{T+h|T} =y_T$.
  * Consequence of efficient market hypothesis.

\pause

### Seasonal naïve method

  * Forecasts equal to last value from same season.
  * Forecasts: $\hat{y}_{T+h|T} =y_{T+h-m(k+1)}$, where $m=$ seasonal period and $k$ is the integer part of $(h-1)/m$.

## Some simple forecasting methods

### Drift method

 * Forecasts equal to last value plus average change.
 * Forecasts:\vspace*{-.7cm}

 \begin{align*}
 \hat{y}_{T+h|T} & =  y_{T} + \frac{h}{T-1}\sum_{t=2}^T (y_t-y_{t-1})\\
                 & = y_T + \frac{h}{T-1}(y_T -y_1).
 \end{align*}\vspace*{-0.2cm}

   * Equivalent to extrapolating a line drawn between first and last observations.

## Some simple forecasting methods

```{r beerf, warning=FALSE, message=FALSE, echo=FALSE, fig.height=4.6}
# Set training data from 1992 to 2007
train <- aus_production %>%
  filter(between(year(Quarter), 1992, 2007))

# Fit the models
beer_fit <- train %>% 
  model(
    Mean = MEAN(Beer),
    `Naïve` = NAIVE(Beer), 
    `Seasonal naïve` = SNAIVE(Beer)
  )

# Produce forecasts
beer_fc <- beer_fit %>% 
  forecast(h=11)

# Plot some forecasts
beer_fc_plot <- beer_fc %>% 
  autoplot(train, level = NULL) +
  ggtitle("Forecasts for quarterly beer production") +
  xlab("Year") + ylab("Megalitres") +
  guides(colour=guide_legend(title="Forecast"))
beer_fc_plot
```

## Some simple forecasting methods

```{r fbf,  message=FALSE, warning=FALSE, echo=FALSE, fig.height=4.6}
# Set training data to first 250 days
fb_stock <- gafa_stock %>% 
  mutate(trading_day = row_number()) %>%
  update_tsibble(index=trading_day, regular=TRUE) %>%
  filter(Symbol == "FB",
         between(Date, ymd("2018-01-01"), ymd("2018-09-01")))

# Plot some forecasts
fb_stock %>% 
  model(
    Mean = MEAN(Close),
    `Naïve` = NAIVE(Close), 
    Drift = RW(Close ~ drift())
  ) %>% 
  forecast(h=42) %>% 
  autoplot(fb_stock, level = NULL) +
  ggtitle("Facebook closing stock price (daily ending Sep 2018)") +
  xlab("Day") + ylab("") +
  guides(colour=guide_legend(title="Forecast"))
```

## Some simple forecasting methods

  * Mean: `MEAN(y)`
  * Naïve:  `NAIVE(y)`
  * Seasonal naïve: `SNAIVE(y)`
  * Drift: `RW(y ~ drift())`

\pause

### Your turn

 * Use these four functions to produce forecasts for Facebook closing price (`gafa_stock`) and Australian takeaway food turnover (`aus_retail`).
 * Plot the results using `autoplot()`.

# Specifying a model

## Specifying a model

model_fn(t(LHS) ~ specials, extras)

# Transformations

## Variance stabilization

\fontsize{13}{15}\sf

If the data show different variation at different levels of the series, then a transformation can be useful.
\pause

Denote original observations as $y_1,\dots,y_n$ and transformed
observations as $w_1, \dots, w_n$.
\pause

\begin{block}{\footnotesize Mathematical transformations for stabilizing
variation}
\begin{tabular}{llc}
Square root & $w_t = \sqrt{y_t}$ & $\downarrow$ \\[0.2cm]
Cube root & $w_t = \sqrt[3]{y_t}$ & Increasing \\[0.2cm]
Logarithm & $w_t = \log(y_t)$  & strength
\end{tabular}
\end{block}
\pause

Logarithms, in particular, are useful because they are more interpretable: changes in a log value are **relative (percent) changes on the original scale**.

## Variance stabilization

```{r food, echo=FALSE, fig.height=4.6}
food <- aus_retail %>% filter(Industry == "Food retailing") %>% summarise(Turnover = sum(Turnover))
food %>% autoplot(Turnover) +
  xlab("Year") + ylab("") +
  ggtitle("Australian food retailing turnover")
```

## Variance stabilization

```{r food-sqrt1, echo=FALSE, fig.height=4.6}
food %>% autoplot(sqrt(Turnover)) +
  xlab("Year") + ylab("") +
  ggtitle("Square root food retailing turnover")
```

## Variance stabilization

```{r food-cbrt, echo=FALSE, fig.height=4.6}
food %>% autoplot(Turnover^(1/3))+
  xlab("Year") + ylab("") +
  ggtitle("Cube root food retailing turnover")
```

## Variance stabilization

```{r food-log, echo=FALSE, fig.height=4.6}
food %>% autoplot(log(Turnover)) +
  xlab("Year") + ylab("") +
  ggtitle("Log food retailing turnover")
```

## Variance stabilization

```{r food-inverse, echo=FALSE, fig.height=4.6}
food %>% autoplot(-1/Turnover) + 
  xlab("Year") + ylab("") +
  ggtitle("Inverse food retailing turnover")
```

## Box-Cox transformations

Each of these transformations is close to a member of the
family of \textbf{Box-Cox transformations}:
$$w_t = \left\{\begin{array}{ll}
        \log(y_t),      & \quad \lambda = 0; \\
        (y_t^\lambda-1)/\lambda ,         & \quad \lambda \ne 0.
\end{array}\right.
$$\pause

* $\lambda=1$: (No substantive transformation)
* $\lambda=\frac12$: (Square root plus linear transformation)
* $\lambda=0$: (Natural logarithm)
* $\lambda=-1$: (Inverse plus 1)

## Box-Cox transformations

```{r food-anim, cache=TRUE, echo=FALSE}
library(latex2exp)
lambda <- seq(1, -1, by=-0.01)
for(i in seq_along(lambda))
{
  savepdf(paste("elecBC",i,sep=""))
  print(autoplot(food, box_cox(Turnover,lambda[i])) + xlab("Year") +
    ylab("") +
    ggtitle(
      TeX(paste("Transformed Australian food retailing turnover:  $\\lambda =",format(lambda[i],digits=2,nsmall=2),"$"))
    ) +
    scale_y_continuous(breaks=NULL,minor_breaks=NULL) +
    theme(axis.title.y=element_blank(),
          axis.text.y=element_blank(),
          axis.ticks.y=element_blank()))
  endpdf()
}
```

\centerline{\animategraphics[controls,buttonsize=0.3cm,width=12.2cm]{4}{elecBC}{1}{201}}

## Box-Cox transformations

```{r food-bc, echo=TRUE,fig.height=4}
food %>% autoplot(box_cox(Turnover,lambda=1/3))
```

## Box-Cox transformations

* $y_t^\lambda$ for $\lambda$ close to zero behaves like logs.
* If some $y_t=0$, then must have $\lambda>0$
* if some $y_t<0$, no power transformation is possible unless all $y_t$ adjusted by \textbf{adding a constant to all values}.
* Simple values of $\lambda$ are easier to explain.
* Results are  relatively insensitive to  $\lambda$.
* Often no transformation ($\lambda=1$) needed.
* Transformation can have very large effect on PI.
* Choosing $\lambda=0$ is a simple way to force forecasts to be positive

## Box-Cox transformations

```{r food-lambda, echo=TRUE}
food %>% features(Turnover, features = guerrero)
```
\pause

* This attempts to balance the seasonal fluctuations and random variation across the series.
* Always check the results.
* A low value of $\lambda$ can give extremely large prediction intervals.

## Back-transformation

We must reverse the transformation (or \textit{back-transform}) to obtain
forecasts on the original scale.  The reverse Transformations are given
by
$$ y_t = \left\{\begin{array}{ll}
        \exp(w_t),      & \quad \lambda = 0; \\
        (\lambda W_t+1)^{1/\lambda} ,   & \quad \lambda \ne 0.
\end{array}\right.$$

## Back-transformation

```{r food-bt,echo=TRUE,fig.height=3.6}
fc <- food %>% 
  model(SNAIVE(box_cox(Turnover, lambda=1/3))) %>% 
  forecast()
fc %>% autoplot(food)
```

## Back-transformation

```{r elec9,echo=TRUE,fig.height=4}
fc %>% autoplot(filter(food, year(Month) > 2010))
```

## Your turn

Find a Box-Cox transformation that works for the Australian gas production (`aus_production`).

## Bias adjustment

  * Back-transformed point forecasts are medians.
  * Back-transformed PI have the correct coverage.

\pause

**Back-transformed means**

Let $X$ be have mean $\mu$ and variance $\sigma^2$.

Let $f(x)$ be back-transformation function, and $Y=f(X)$.

Taylor series expansion about $\mu$:
$$f(X) = f(\mu) + (X-\mu)f'(\mu) + \frac{1}{2}(X-\mu)^2f''(\mu).$$\pause

\begin{alertblock}{}
\centerline{$\E[Y] = \E[f(X)] = f(\mu) + \frac12 \sigma^2 f''(\mu)$}
\end{alertblock}

## Bias adjustment

\fontsize{13}{15}\sf

**Box-Cox back-transformation:**
\begin{align*}
y_t &= \left\{\begin{array}{ll}
        \exp(w_t)      & \quad \lambda = 0; \\
        (\lambda W_t+1)^{1/\lambda}  & \quad \lambda \ne 0.
\end{array}\right. \\
f(x) &= \begin{cases}
                        e^x & \quad\lambda=0;\\
 (\lambda x + 1)^{1/\lambda} & \quad\lambda\ne0.
 \end{cases}\\
f''(x) &= \begin{cases}
                        e^x & \quad\lambda=0;\\
 (1-\lambda)(\lambda x + 1)^{1/\lambda-2} & \quad\lambda\ne0.
 \end{cases}
\end{align*}\pause
\begin{alertblock}{}
\centerline{$\E[Y] = \begin{cases}
                        e^\mu\left[1+\frac{\sigma^2}{2}\right] & \quad\lambda=0;\\
 (\lambda \mu + 1)^{1/\lambda}\left[1+\frac{\sigma^2(1-\lambda)}{2(\lambda\mu+1)^2}\right] & \quad\lambda\ne0.
 \end{cases}$}
\end{alertblock}

## Bias adjustment

\fontsize{10}{10}\sf

```{r biasadj, fig.height=3}
eggs <- as_tsibble(fma::eggs)
fit <- eggs %>% model(RW(log(value) ~ drift()))
fc <- fit %>% forecast(h=50)
fc_biased <- fit %>% forecast(h=50, bias_adjust = FALSE)
eggs %>% autoplot(value) +
  autolayer(fc_biased, series="Simple back transformation", level = 80) +
  autolayer(fc, series="Bias adjusted", level = NULL) +
  guides(colour=guide_legend(title="Forecast"))
```

# Distributional forecasts

## Prediction intervals

 * A forecast $\hat{y}_{T+h|T}$ is (usually) the mean of the conditional distribution $y_{T+h} \mid y_1, \dots, y_{T}$.
 * A prediction interval gives a region within which we expect $y_{T+h}$ to lie with a specified probability.
 * Assuming forecast errors are normally distributed, then a 95% PI is
 \begin{alertblock}{}
\centerline{$
  \hat{y}_{T+h|T} \pm 1.96 \hat\sigma_h
$}
\end{alertblock}
where $\hat\sigma_h$ is the st dev of the $h$-step distribution.

 * When $h=1$, $\hat\sigma_h$ can be estimated from the residuals.

## Prediction intervals

\small

**Naive forecast with prediction interval:**

```{r googpi, echo=TRUE, cache=TRUE, eval = FALSE}
res_sd <- sqrt(mean(res^2, na.rm=TRUE))
c(tail(google_2015,1)) + 1.96 * res_sd * c(-1,1)
```

```{r googforecasts, echo=TRUE, cache=TRUE, eval = FALSE}
naive(google_2015, level=95)
```

## Prediction intervals

 * Point forecasts are often useless without prediction intervals.
 * Prediction intervals require a stochastic model (with random errors, etc).
 * Multi-step forecasts for time series require a more sophisticated approach (with PI getting wider as the forecast horizon increases).

## Prediction intervals

\fontsize{14}{18}\sf

Assume residuals are normal, uncorrelated, sd = $\hat\sigma$:

\begin{block}{}
\begin{tabular}{ll}
\bf Mean forecasts: & $\hat\sigma_h = \hat\sigma\sqrt{1 + 1/T}$\\[0.2cm]
\bf Naïve forecasts: & $\hat\sigma_h = \hat\sigma\sqrt{h}$\\[0.2cm]
\bf Seasonal naïve forecasts & $\hat\sigma_h = \hat\sigma\sqrt{k+1}$\\[0.2cm]
\bf Drift forecasts: & $\hat\sigma_h = \hat\sigma\sqrt{h(1+h/T)}$.
\end{tabular}
\end{block}

where $k$ is the integer part of $(h-1)/m$.

Note that when $h=1$ and $T$ is large, these all give the same approximate value $\hat\sigma$.

## Prediction intervals

  * Computed automatically from the forecast distribution.
  * Use `level` argument to control coverage.
  * Check residual assumptions before believing them.
  * Usually too narrow due to unaccounted uncertainty.
